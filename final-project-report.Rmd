---
title: 'STAT 471: Final Project'
author: 'Leontij Potupin, Rachel Wu'
date: 'December 8, 2021'
output:
  bookdown::pdf_document2:
    number_sections: yes
    toc: yes
    toc_depth: '3'
  html_document:
    highlight: haddock
    number_sections: yes
    theme: lumen
    toc: no
    toc_depth: 4
    toc_float: yes
urlcolor: blue
---

## Introduction

## Downloading Data
```{r, message = FALSE}
library(keras)         # to train neural networks
library(kableExtra)    # to print tables
library(cowplot)       # to print side-by-side plots
library(tidyverse)     # tidyverse
library(tensorflow)
library(dplyr)
library(haven)
library(pROC) # for ROC curves
library(ggplot2)
library(haven)
library(corrplot)
library(ggcorrplot)
library(glmnetUtils)
```

```{r}

url_2019 = "https://ftp.cdc.gov/pub/Health_Statistics/NCHS/dataset_documentation/nhamcs/stata/ED2019-stata.zip"
url_2018 = "https://ftp.cdc.gov/pub/Health_Statistics/NCHS/dataset_documentation/nhamcs/stata/ED2018-stata.zip"
url_2017 = "https://ftp.cdc.gov/pub/Health_Statistics/NCHS/dataset_documentation/nhamcs/stata/ed2017-stata.zip"
url_2016 = "https://ftp.cdc.gov/pub/Health_Statistics/NCHS/dataset_documentation/nhamcs/stata/ED2016-stata.zip"
url_2015 = "https://ftp.cdc.gov/pub/Health_Statistics/NCHS/dataset_documentation/nhamcs/stata/ED2015-stata.zip"
temp = tempfile()
download.file(url_2019,temp)
data_2019 = read_dta(unz(temp,"ED2019-stata.dta"))

temp = tempfile()
download.file(url_2018,temp)
data_2018 = read_dta(unz(temp,"ED2018-stata.dta"))

temp = tempfile()
download.file(url_2017,temp)
data_2017 = read_dta(unz(temp,"ed2017-stata.dta"))

temp = tempfile()
download.file(url_2016,temp)
data_2016 = read_dta(unz(temp,"ED2016-stata.dta"))

temp = tempfile()
download.file(url_2015,temp)
data_2015 = read_dta(unz(temp,"ED2015-stata.dta"))

head(data_2019)
head(data_2018)
head(data_2017)
head(data_2016)
head(data_2015)
```

## Data Wrangling
```{r}
data_raw = bind_rows(data_2015, data_2016, data_2017, data_2018, data_2019)
head(data_raw)
data_raw
```
### Clean missing data and delete variables with too few entries
```{r}
#THINGS TO DO: CTCONTRAST LEAVE IN OR OUT, DIAG2R TAKE OUT -9 ROWS OR SCRAP WHOLE COLUMN?
      #Do we keep PRDIAG2 & 3? check with
                #nrow(data_clean_delete_var[data_clean_delete_var$PRDIAG2==-7, ])
      #Fill in means line 162

#Notes: -7 codes for Not Applicable, -9 codes for Blank, "Blank"

#order columns alphabetically
data_raw_ordered = data_raw[,order(colnames(data_raw))]
#Impute missing data
data_clean_delete_missing <- data_raw_ordered[!(data_raw_ordered$ADMDIV < 0 | #controversial to keep
                                                  data_raw_ordered$AMBTRANSFER < 0 | #controversial to keep
                                                  data_raw_ordered$RESIDNCE < 0 |
                                                  data_raw_ordered$RESPR < 0 |
                                                  data_raw_ordered$TEMPF < 0 |
                                                  data_raw_ordered$TEMPDF < 0 |
                                                  data_raw_ordered$PULSE < 0 |
                                                  data_raw_ordered$TOTDIAG < 0 |
                                                  data_raw_ordered$TOTPROC < 0 |
                                                  data_raw_ordered$WAITTIME < 0 |
                                                  data_raw_ordered$WAITTIME < 0) |
                                                  !is.na(data_raw_ordered$PRESCR1) |
                                                  !is.na(data_raw_ordered$PRESCR2) |
                                                  !is.na(data_raw_ordered$CONTSUB1) |
                                                  !is.na(data_raw_ordered$CONTSUB2), ]

#Delete all the char variables, will not work in the glm_fit, cuts col from 1058 to 521
data_clean_delete_char <- data_clean_delete_missing[, sapply(data_clean_delete_missing, 
                                                             function(x) !is.character(x))]
#Delete certain variables taken post/at discharge
data_clean_delete_var <- data_clean_delete_char %>% 
  select(-c(AGER, AGEDAYS, ADISP, LOS,
            RFV4, RFV5, 
            RFV43D, RFV53D, 
            PRDIAG4, PRDIAG5,
            MRICONTRAST, MED10:MED30,
            GPMED4:GPMED30,
            LEFTBTRI:DIEDED, ADMIT, ADMTPHYS, AGEFL, BLANK1, BLANK2, 
            BLANK3, BLANK4, CPSUM, CSTRATM, COMSTAT1:COMSTAT9,EDWT, 
            HOSPCODE, LBTC, LWBS, MSA, OBSDIS, OBSHOS, 
            PATCODE, PATWT, PULSED, RACERFL, RESPRD, PTONLINEE1: PTONLINEE6,
            RFID, SEXFL, SETTYPE, TEMPDF, TRANNH, TRANOTH, TRANPSYC, VITALSD , YEAR,
            BPDIASD, BPSYSD))

#Delete these variables with >90% NA or -9, no way to impute, ncol/variables 234 to 179
#note SURGDAY is obsolete variable in 2018, so deleting
data_clean_delete_var <- data_clean_delete_var %>%
  select(-c(BOARDED, CAUSE1R:CAUSE3R,
            CONTSUB10:CONTSUB19, CONTSUB20:CONTSUB9, 
            DIAG3R:DIAG5R, PRESCR10:PRESCR19,
            PRESCR20:PRESCR29, PRESCR6:PRESCR9, SURGDAY))

#Delete these variables with 50% NA or -9
data_delete_var <- data_clean_delete_var %>% 
  select(-c(MED4:MED9, PRESCR3:PRESCR5, REGDIV, TOTHRDIVR))
```
### Create factors from doubles
```{r}
data_factored <- data_delete_var
keep_double <- c("AGE", "BPDIAS", "BPSYS", 
                "NUMDIS", "NUMGIV", "NUMMED", "OBSSTAY", 
                 "PAINSCALE", "POPCT", "PULSE", "RESPR", 
                 "TOTCHRON", "TOTDIAG", "TOTPROC", "WAITTIME")

col_names <- names(data_factored)
data_factored[,col_names] <- lapply(data_factored[,col_names] , as.factor)

data_factored[ ,keep_double] <- lapply(data_factored[ , keep_double], as.numeric)

# job_clean <- job_satisfaction %>% mutate(income = ordered(Income, 
#         c(levels=c("<5000", "5000-15000", "15000-25000", ">25000")))) %>% 
#   mutate(job = ordered(Satisfaction,
#         levels=c("Very Dissatisfied", "A Little Satisfied", 
#                  "Moderately Satisfied", "Very Satisfied"))) %>%
#   mutate(Gender = ordered(Gender, levels=c("Female", "Male")))

```
### Imputing means
```{r}
#Find the means for each column for imputing
means <- apply(data_factored[,keep_double], 2, function(x) {
  mean(x[which(x > 0)])
  })
pain_mean <- as.integer(mean(data_factored$PAINSCALE[which(data_factored$PAINSCALE < 11)]))
#for loop to fill keep_double 
data_factored$AGE[is.na(data_factored$AGE) | data_factored$AGE < 0] <- means["AGE"]
data_factored$BPDIAS[is.na(data_factored$BPDIAS) | data_factored$BPDIAS < 2] <- means["BPDIAS"]
data_factored$BPSYS[is.na(data_factored$BPSYS) | data_factored$BPSYS < 2] <- means["BPSYS"]
#data_factored$NUMDIS[is.na(data_factored$NUMDIS) | data_factored$NUMDIS < 0] <- means[4]
#data_factored$NUMGIV[is.na(data_factored$NUMGIV) | data_factored$NUMGIV < 0] <- means[5]
#data_factored$NUMMED[is.na(data_factored$NUMMED) | data_factored$NUMMED < 0] <- means[5]
data_factored$PAINSCALE[data_factored$PAINSCALE > 10] <- pain_mean
data_factored$POPCT[is.na(data_factored$POPCT) | data_factored$POPCT < 0] <- means["POPCT"]
data_factored$PULSE[is.na(data_factored$PULSE) | data_factored$PULSE < 0] <- means["PULSE"]
data_factored$RESPR[is.na(data_factored$RESPR) | data_factored$RESPR < 0] <- means["RESPR"]
data_factored$TOTCHRON[is.na(data_factored$TOTCHRON) | data_factored$TOTCHRON < 0] <- means["TOTCHRON"]
data_factored$TOTDIAG[is.na(data_factored$TOTDIAG) | data_factored$TOTDIAG < 0] <- means["TOTDIAG"]
data_factored$TOTPROC[is.na(data_factored$TOTPROC) | data_factored$TOTPROC < 0] <- means["TOTPROC"]
data_factored$WAITTIME[is.na(data_factored$WAITTIME) | data_factored$WAITTIME < 0] <- means["WAITTIME"]

#################For testing
#nrow(data_clean_delete_var[data_clean_delete_var$LOV==-7, ])
#sum(is.na(data_clean_delete_var$CONTSUB16))
#sapply(lapply(data_clean_delete, unique), length)
#sapply(data_factored, typeof)
#Number of variables left
#ncol(data_delete_var)
```
### Giving rows with NA another factor level
```{r}
#adding 0 level to CONTSUB1, CONTSUB2, PRESC1, PRESC2, for NA rows then refactoring 
data_factored$CONTSUB1[is.na(data_factored$CONTSUB1)] <- 0
data_factored$CONTSUB2[is.na(data_factored$CONTSUB2)] <- 0

#Unfactor PRESCR1 and 2
data_factored$PRESCR1 <- as.numeric(data_factored$PRESCR1)
data_factored$PRESCR1[is.na(data_factored$PRESCR1)] <- 0
data_factored$PRESCR2 <- as.numeric(data_factored$PRESCR2)
data_factored$PRESCR2[is.na(data_factored$PRESCR2)] <- 0
data_factored$PRESCR1 <- as.factor(data_factored$PRESCR1)
data_factored$PRESCR2 <- as.factor(data_factored$PRESCR2) 

# dropping rows with NA values
data_factored = data_factored %>%
  drop_na()

```

Removing empty columns, remove gpmed variables because numGiv gives you sum of # gpmed per patient, removing LEFTBTRI:DIEDED because these patients are not admitted into the ED (therefore cannot be admitted to hospital) or they died before they could be admitted to hosptial (dead on arrival or died in the ED).

### Train & Test Split
```{r}
train_samples = sample(1:nrow(data_factored), 0.8*nrow(data_factored))
admit_train = data_factored %>% filter(row_number() %in% train_samples)
admit_test = data_factored %>% filter(!(row_number() %in% train_samples))

# Keep only first 50 attributes in training data to make model not too complex
admit_train = admit_train[1:50]
```

## Data Description and Exploration
**Our response variable is ADMITHOS, which is a categorical variable assigned a 1 if the patient was admitted to this hospital and 0 if they were not admitted. We have several similar variables that tell us information on patient disposition, ie where a patient went after appearing at the hospital. OBSHOS (indicating patients were admitted to the hospital after observation in the emergency department), TRANPYSC (patient transferred to a psychiatric hospital), and TRANOTH (patient was transferred to another hospital) were also variables that indicate the patient experienced additional care after being seen in the emergency department. However, we decided to focus only on ADMITHOS because of the clarity of the variable compared to the other potential response variables. TRANPYSC could indicate the patient went to a pyschiatric hospital but may not have been admitted to that hospital; the same concern plagues the TRANOTH variable. OBSHOS is also binary and for every positive OBSHOS observed, there is also a positive ADMITHOS value for that given patient, so we**
```{r}
admit_train %>%
  group_by(ADMITHOS) %>%
  summarise(cnt = n()) %>%
  mutate(freq = round(cnt / sum(cnt), 3))
```
**Around 10.9% of people were admitted to the hospital.**
```{r}
# correlation matrix
corr_simple <- function(data=df,sig=0.3){
  #convert data to numeric in order to run correlations
  #convert to factor first to keep the integrity of the data - each value will become a number rather than turn into NA
  df_cor <- data %>% mutate_if(is.character, as.factor)
  df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
  #run a correlation and drop the insignificant ones
  corr <- cor(df_cor)
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > sig) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  #print table
  print(corr)
  #turn corr back into matrix in order to plot with corrplot
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
  #plot correlations visually
  corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}

corr_simple(admit_train)
```
## Model Building, Evaluation, and Interpretation

### Logistic Regression
```{r}
admit_train
```

```{r}
glm_fit = glm(ADMITHOS ~ .,
              family = "binomial",
              data = admit_train, na.action = na.exclude)

coef(glm_fit)

fitted_probabilities_glm = predict(glm_fit,
        newdata = admit_test,
        type = "response") # to get output on probability scale head(fitted_probabilities)

predictions_glm = as.numeric(fitted_probabilities_glm > 0.5)

admit_test_pred = admit_test %>%
  mutate(predicted_admit_glm = predictions_glm)

# then calculate misclassification rate
admit_test_pred %>%
    summarise(mean(ADMITHOS != predicted_admit_glm))

#confusion matrix
admit_test_pred %>%
  select(ADMITHOS, predicted_admit_glm) %>%
  table()

```
```{r}
# ROC Curve
roc_data = roc(admit_test %>% pull(ADMITHOS),
               fitted_probabilities)
tibble(FPR = 1-roc_data$specificities,
       TPR = roc_data$sensitivities) %>%
  ggplot(aes(x = FPR, y = TPR)) +
  geom_line() +
  geom_abline(slope = 1, linetype = "dashed") +
#  geom_point(x = fpr, y = 1-fnr, colour = "red") +
  theme_bw()

# print the AUC
roc_data$auc
 
```
```{r}
admit_train %>%
  ggplot(aes(x = CBC, y = ADMITHOS))+
  geom_jitter(height = .05) +
  geom_smooth(method = "glm",
              formula = "y~x",
              method.args = list(family = "binomial"),
              se = FALSE) +
  ylab("Prob(ADMITHOS=1)") +
  theme_bw()
```

### Ridge logistic regression

```{r}
ridge_fit = cv.glmnet(ADMITHOS ~ .,
                      alpha = 0, # alpha = 0 means ridge
                      nfolds = 10, # number of CV folds
                      family = "binomial", # to specify logistic regression
                      type.measure = "class", # use misclassification error in CV 
                      data = admit_train) # train on admit_train data
plot(ridge_fit)
plot_glmnet(ridge_fit, admit_train, features_to_plot = 4)
```
```{r}
# Making predictions using ridge logistic regression
fitted_probabilities_ridge = predict(ridge_fit,
        newdata = admit_test,
        s = "lambda.1se",
        type = "response") # to get output on probability scale 
head(fitted_probabilities_ridge)

predictions_ridge = as.numeric(fitted_probabilities_ridge > 0.5)

admit_test_pred = admit_test %>%
  mutate(predicted_admit_ridge = predictions_ridge)

# then calculate misclassification rate
admit_test_pred %>%
    summarise(mean(ADMITHOS != predicted_admit_ridge))
```

### Lasso logistic regression

```{r}
lasso_fit = cv.glmnet(ADMITHOS ~ ., # formula notation, as usual
                      alpha = 1, # alpha = 1 for lasso
                      nfolds = 10, # number of folds
                      family = "binomial", # to specify logistic regression
                      type.measure = "class", # use misclassification error in CV 
                      data = admit_train) # data to run lasso on
```

```{r}
plot(lasso_fit)
plot_glmnet(lasso_fit, admit_train, features_to_plot = 4)
```

```{r}
# Making predictions
lasso_predictions = predict(lasso_fit, 
                            newdata = admit_test,
                            s = "lambda.1se") %>% as.numeric()
lasso_predictions = as.numeric(lasso_predictions > 0.5)

admit_test_pred = admit_test %>%
  mutate(predicted_admit_lasso = lasso_predictions)

# then calculate misclassification rate
admit_test_pred %>%
    summarise(mean(ADMITHOS != predicted_admit_lasso))
```

### Elastic net regression

```{r}
elnet_fit = cv.glmnet(ADMITHOS ~ ., # formula notation, as usual
                      nfolds = 10, # number of folds
                  #    family = "binomial", # to specify logistic regression
                    #  type.measure = "class", # use misclassification error in CV 
                      data = admit_train) # data to run elnet on
elnet_fit$alpha
plot_cva_glmnet(elnet_fit)
elnet_fit_best = extract_best_elnet(elnet_fit)
elnet_fit_best$alpha
```

```{r}
plot(elnet_fit_best)
plot_glmnet(elnet_fit_best, admit_train, features_to_plot = 6)
```

```{r}
# Making predictions
elnet_predictions = predict(elnet_fit, 
                            newdata = admit_test,
                            s = "lambda.1se") %>% as.numeric()
elnet_predictions = as.numeric(elnet_predictions > 0.5)

admit_test_pred = admit_test %>%
  mutate(predicted_admit_elnet = elnet_predictions)

# then calculate misclassification rate
admit_test_pred %>%
    summarise(mean(ADMITHOS != predicted_admit_elnet))
```

### Trees

### CNN

## Conclusions

## Appendix

